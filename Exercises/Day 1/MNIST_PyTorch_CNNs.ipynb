{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["In this notebook, we will implement a Convoulutional Neural Network (CNN) using pytorch for MNIST Classification."],"metadata":{"id":"4Eo1RFp4AmuA"}},{"cell_type":"markdown","source":["Expectations: Please provide solutions to the questions in the cells at the end of the notebook."],"metadata":{"id":"_-O8HVTpkbZm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vqg8SEXq55vV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import models,transforms\n","from torchvision.utils import make_grid\n","from torchvision.datasets import MNIST\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","source":["We will be using MNIST Disgits datasets. The MNIST Digits dataset consists of 70000 28x28 grayscale images of digits from 0 to 9, with 6000 images per class. There are 60000 training images and 10000 test images. <br>\n","\n","Following are the some random samples from the dataset.\n","\n","![MNIST Samples](https://www.yunzhew.com/project/mnist-digit-net/featured_hudee2c27f78ea2485e0d3aa44abbfc53c_218555_720x2500_fit_q75_h2_lanczos_3.webp)"],"metadata":{"id":"dSlDpJqVBLoL"}},{"cell_type":"markdown","source":["We will use pytorch datasets to fetch the MNIST Digits dataset as it provides a handy way to get and use the dataset. More information about pytorch datasets [here](https://pytorch.org/vision/stable/datasets.html)."],"metadata":{"id":"zK94z-ilDh0l"}},{"cell_type":"code","source":["batch_sz=64 # this is batch size i.e. the number of rows in a batch of data\n","\n","train_dataset = MNIST(root='./datasets', train=True, download=True, transform = transforms.ToTensor())\n","test_dataset = MNIST(root='./datasets', train=False, download=True, transform = transforms.ToTensor())\n","\n","train_loader = DataLoader(train_dataset, batch_size = batch_sz)\n","test_loader = DataLoader(test_dataset, batch_size = batch_sz)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AQyKnqBbm9X","executionInfo":{"status":"ok","timestamp":1684310815569,"user_tz":-180,"elapsed":950,"user":{"displayName":"Naeemullah Khan","userId":"18307185785900481812"}},"outputId":"90c5d277-7328-4e0a-efc8-d9458305f187"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 104112364.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 69418735.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 28494698.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 22333562.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n","\n"]}]},{"cell_type":"code","source":["len(train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ9DoKj7b9oE","executionInfo":{"status":"ok","timestamp":1684310815569,"user_tz":-180,"elapsed":3,"user":{"displayName":"Naeemullah Khan","userId":"18307185785900481812"}},"outputId":"7f66c719-8b87-445f-d537-eb3a8d642523"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["938"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Convolutional Neural Networks"],"metadata":{"id":"u65mV2e_2H_8"}},{"cell_type":"markdown","source":["Now, we will construct a Convolutional Neural Network. Convolutional neural networks are a type of neural networks which are typically applied to image data. They work by convolving a filter on an image. Filters act as weights of CNN and we learn these filters to extract useful information from an image. Filters are also sometimes called kernels.\n","<br>\n","\n","We element-wise muliply a filter with a patch of input data and then sum the result.<br>\n","$$z_{ij} = W \\star x_{ij} = \\sum^{m-1}_{a=0}\\sum^{m-1}_{b=0} W_{ab} \\: x_{(i+a)(j+b)}$$\n","![convolution](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif) [source](https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif)\n","\n","<br>\n","\n","A convolutional layer in a CNN consists of a number applying a number of such filters to the input. The output of each of these filters is stacked in the form of multiple channels (just like we have 3 channesl in an RGB image). The filters then also become 3-dimensional.\n","\n","![2d conv](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oFVlkvZp848nh-QoD3pREw.png) [source](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)"],"metadata":{"id":"_nQ-MHGI2MuW"}},{"cell_type":"markdown","source":["### Stride"],"metadata":{"id":"4ooJgob8AywM"}},{"cell_type":"markdown","source":["When we are dealing with convolution on images of very large size, its not always required to convolve over each and every pixel of an image. So we can set the subsequent convolutions to be shifted by more than one pixel in either the vertical or horizontal axis. This shift in subsequent convolutions is called the stride.\n","\n","![stride](https://miro.medium.com/v2/resize:fit:1400/1*BNLPHcNxLCgtwlJHnSs9oA.gif) [source](https://medium.com/swlh/convolutional-neural-networks-part-2-padding-and-strided-convolutions-c63c25026eaa)"],"metadata":{"id":"dv8twKrxA1LI"}},{"cell_type":"markdown","source":["### Pooling"],"metadata":{"id":"PT-HLosN9unN"}},{"cell_type":"markdown","source":["The pooling operation involves sliding a two-dimensional filter over each channel of feature map and summarising the features lying within the region covered by the filter. \n","\n","Pooling layers are used to reduce the dimensions of the feature maps and summarising the featre maps.\n","\n","Types of Pooling Layers:\n","1.   Max Pooling: <br>\n","Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. This is the most commonly used pooling layer.\n","![Max Pooling](https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png) [source](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)\n","\n","\n","2.   Average Pooling: <br>\n","Average pooling computes the average of the elements present in the region of feature map covered by the filter.\n","![Avg Pooling](https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png) [source](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)\n","\n"],"metadata":{"id":"spWnWvfo9w8a"}},{"cell_type":"markdown","source":["### Convolutional Neural Network\n","A common CNN model architecture is to have a number of convolution and pooling layers stacked one after the other.\n","![cnn](https://indiantechwarrior.com/wp-content/uploads/2021/04/LeNet-1024x393.png) [source](https://indiantechwarrior.com/convolutional-neural-network-architecture/)\n","\n"],"metadata":{"id":"cdzVnj6vC2Em"}},{"cell_type":"markdown","source":["We will use the categorical crossentropy loss here which is traditionally used for classification.\n","\n","$$\\mathcal{L} = \\frac{1}{N} \\sum^{N}_{i=1} \\left ( \\sum^{C}_{j=1} -y_{i,j}\\: log(\\hat{y}_{i,j}) \\right )$$\n","\n","where $y_i,j$ is the groundtruth, $\\hat{y}_{i,j}$ is the prediction, $C$ is the number classes and $N$ is the number of data samples."],"metadata":{"id":"LXe05pWKDxgk"}},{"cell_type":"code","source":[],"metadata":{"id":"xdQbo2bm8jZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q1: define a 2 layer simple NN for mnist digit classificaiton"],"metadata":{"id":"0__ql-1B6f-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q2: Define a CNN with 2 conv layer and 2 linear layers for mnist digit classification"],"metadata":{"id":"qPzVoKv26mqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q3: Train both networks for 10 epochs and compare their performance "],"metadata":{"id":"4tF40n3B668b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q4: Compare the accuarcay of both networks on the test set"],"metadata":{"id":"x0AauPBu7mQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q5: go through the testset and plot some samples of incorrect results"],"metadata":{"id":"_-cN-QfD7uoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q6: show the output of the intermediate layers"],"metadata":{"id":"38vMW-mI72cE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Q7: Compare the time of training on CPU and GPU"],"metadata":{"id":"PGRQUcd0i22U"},"execution_count":null,"outputs":[]}]}